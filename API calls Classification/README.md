# Malware Classification using API Calls

## Abstract
As the usage of computers in daily life increases, computer attackers are forced to use a variety of strategies to attack computers or even use them as weapons. With each new operating system version or update, computers grow more secure, but there are a variety of ways that hackers can get around these security measures. The most typical scenario of security component bypass techniques involves malware that modifies its behavior and source code on every infected PC. Malware analysis refers to all techniques used by analysts to find harmful software. Malware analysis is a wide concept with numerous stages. These steps include inspecting the suspect software's code without executing it, running it in a controlled environment, investigating DNS resolution requests, logging registry reads and writes, file accesses, and application programming interface (API) calls. The use of operating system API calls is a promising task in detecting PE-type malware in the Windows operating system. So, the data that is used in this project is a public malware dataset generated by Cuckoo sandbox based on Windows operating system API calls analysis. Itâ€™s about 7107 calls for 8 malware families: Trojan, Backdoor, Downloader, Worms, Spyware, Adware, Dropper, and Virus. We used several machine learning algorithms in this project to classify these eight families. The steps we followed to accomplish our goals are preprocessing the data and performing feature reduction using PCA. Then, classifying the API calls using six machine learning models which are Logistic Regression (LR), K nearest neighbor (KNN), support vector machine (SVM), random forest (RF), decision tree (DT), and hist gradient boosting (HGB). finally, using Grid search for hyperparameter tuning to find the best combination between hyperparameters. The best model was Hist gradient boosting using default hyperparameters with an accuracy of 41%. The reason for low performance may be the dataset itself because the creator of the dataset used a VirusTotal that uses 66 different antivirus applications for file analysis. They observed that not every antivirus application can detect the same malware. So, they assigned the API calls according to the majority class of all analyses

## Methodology
### A. Dataset
Our dataset contains of 1000 calls and 1000 types which are translated the families produced by each of the software into 8 main malware families: Trojan, Backdoor, Downloader, Worms, Spyware Adware, Dropper, Virus. Table 1 shows the number of malwares belonging to malware families in our data set. As you can see in the table, the number of samples of other malware families except AdWare is quite close to each other. There is such a difference because we don't find too much of malware from the adware malware family.

| Malware| Sample| Description |
| ---    | ---   | ---         |
| Spyware | 832 | enables a user to obtain covert information about another's computer activities by transmitting data covertly from their hard drive |
| Downloader | 1001 | share the primary functionality of downloading content |
| Trojan | 1001 | misleads users of its true intent |
| Worms | 1001 | spreads copies of itself from computer to computer |
| Adware | 379 | hides on your device and serves you advertisements |
| Dropper | 891 | surreptitiously carries viruses, back doors and other malicious software so they can be executed on the compromised machine |
| Virus | 1001 | designed to spread from host to host and has the ability to replicate itself |
| Backdoor | 1001 | a technique in which a system security mechanism is bypassed undetectably to access a computer or its data
| Total | 7107| |

### B. Modeling
There is such a difference because we don't find too much of malware from the adware malware family. The purposed techniques are used to classify the 8 families of the malware. Different machine learning algorithms, such as Logistic Regression, Decision trees, Support vector Machine, Hist gradient boosting, Random forest and KNN are compared to see which one be best suit the situation and can be used on our data efficiently.

These are the steps for selecting the best model proceed.
1. Read the dataset
2. Preprocessing on the data by checking for nulls then splitting sequences and adding padding then applying the label encoder and splitting the data into training and testing sets by train test split with providing 80% of the data for training and 20% testing.
3. Performing Exploratory Data Analysis (EDA) which is an approach to analyze the data using
visual techniques. It is used to discover trends, patterns, or to check assumptions with the help of statistical summary and graphical representations. It allows a machine learning model to predict our dataset better. Gives you more accurate results. It also helps us to choose a better machine learning model.
3.1. First, Showing the distribution of the labels.

![](https://github.com/TokaMamdoh/Cyber_Security/blob/7a6f902d2a52c973b47f17b81b6b327e2dd077ed/API%20calls%20Classification/images/Class%20Distribution.PNG)

3.2. Applying dimensionality reduction and choosing baseline model which was the Logistic Regression. It achieved 30% accuracy

![](https://github.com/TokaMamdoh/Cyber_Security/blob/7a6f902d2a52c973b47f17b81b6b327e2dd077ed/API%20calls%20Classification/images/Classification%20Report%20of%20the%20baseline%20model.PNG)

3.3. Applying PCA (Principal component Analysis) to enable us to visualize multidimensional data and reduce the dimensionality (number of features) within a dataset while still retaining as much information as possible. It helped us to produce variety of the accuracy to our model after applying it. The accuracy after applying PCA on data was 30.4%. This figure shows number of features and accuracy.

![](https://github.com/TokaMamdoh/Cyber_Security/blob/7a6f902d2a52c973b47f17b81b6b327e2dd077ed/API%20calls%20Classification/images/No.%20of%20features%20and%20accuracy.PNG)

4. Models' Training Step

   4.1. Appling multiple models to try on the data which are LR, DT, SVM, HG, RF and KNN 
   
        - Logistic Regression (LR): is a classification algorithm. It is used to predict a binary outcome based on a set of independent variables.

        - Decision Tree (DT): is the most powerful and popular tool for classification and prediction. A Decision tree is a flowchart-like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label.
        
        - Support Vector Machine (SVM): is a supervised machine learning algorithm used for both classification and regression. Though we say regression problems as well its best suited for classification. The objective of SVM algorithm is to find a hyperplane in an N-dimensional space that distinctly classifies the data points. The dimension of the hyperplane depends upon the number of features.
        
        - Hist Gradient Boosting (HG): is an ensemble machine learning algorithm. Boosting refers to a class of ensemble learning algorithms that add tree models to an ensemble sequentially.
        
        - Random Forest (RF): a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.
        
        - K-Nearest Neighbor (KNN): is a type of supervised machine learning algorithm used to solve classification and regression problems. However, it's mainly used for classification problems.

   4.2. Then evaluating each model in turn and applying Kfold cross-validator which split dataset into k consecutive folds (without shuffling by default). Each fold is used once as a validation while the k
       
       - 1 remaining folds form the training set. Read more in the User Guide.
    
   4.3. Comparing the accuracies of the models and choosing the highest two at least which were Random Forest Classifier and Hist Gradient Boosting Classifier.

   4.4. Applying Grid search to find the best hyperparameter of Random Forest model then extracted the best model which was Hist Gradient Boosting Classifier and best parameters.

5. Evaluation Step: Making an evaluation for the champion model which was Hist Gradient Boosting. It achieved the highest accuracy which is 41.2%

![](https://github.com/TokaMamdoh/Cyber_Security/blob/7a6f902d2a52c973b47f17b81b6b327e2dd077ed/API%20calls%20Classification/images/Classification%20Report%20of%20the%20champion%20model.PNG)
